{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation\n",
    "```bash\n",
    "# create an environment\n",
    "conda create --name lec4 python=3.9\n",
    "conda activate lec4\n",
    "# install pytorch. This one can use GPU acceleration on mac\n",
    "conda install pytorch -c pytorch-nightly \n",
    "# install jupyter\n",
    "conda install -n lec4 ipykernel --update-deps --force-reinstall\n",
    "conda install -c anaconda jupyter\n",
    "# installing huggingface libraries\n",
    "conda install transformers\n",
    "conda install datasets\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walkthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset piqa (C:\\Users\\cjj90\\.cache\\huggingface\\datasets\\piqa\\plain_text\\1.1.0\\6c611c1a9bf220943c4174e117d3b660859665baf1d43156230116185312d011)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e271020908a542c1bc53e0af7c8a357c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# piqa is a physical reasoning question answer dataset that provides two potential (and usually similar)\n",
    "# responses to each question as well as a label for the right answer\n",
    "dataset = load_dataset(\"piqa\")\n",
    "train = dataset['train']\n",
    "# using validation because test appears to not be labelled\n",
    "test = dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function consolidate at 0x0000025F890F4160> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86b0c20f9f11406e9cb0c22d3998780c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16113 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dba72dfa14394b3eaa7807f5cf7f8fd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1838 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# only hold onto the right solution\n",
    "def consolidate(row):\n",
    "    return {'sol': (row['sol1'] if row['label'] == 0 else row['sol2'])}\n",
    "train = train.map(consolidate, remove_columns = ['label', 'sol1', 'sol2'])\n",
    "test = test.map(consolidate, remove_columns = ['label', 'sol1', 'sol2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'goal': \"When boiling butter, when it's ready, you can\",\n",
       " 'sol': 'Pour it into a jar'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    }
   ],
   "source": [
    "# load config without weights\n",
    "model_name = \"bert-base-uncased\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d8808ff54bd41deb4a856695c66031d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be9bedee5eb54d39baa3b8ae88f0bd10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['goal'], examples['sol'], truncation=True)\n",
    "\n",
    "tokenized_train = train.map(tokenize_function, batched = True, remove_columns = ['goal', 'sol'])\n",
    "tokenized_test = test.map(tokenize_function, batched = True, remove_columns = ['goal', 'sol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128\n",
    "\n",
    "def group_texts(examples):\n",
    "    # repeat concatenation for input_ids and other keys\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in\n",
    "                            examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "\n",
    "    # populate each of input_ids and other keys \n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0,\n",
    "            total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    # add labels because we'll need it as the output\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79964953b7b4a1cbe9da8866fa86869",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18ec78736f3749e98f788a9e8774f52c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_train = tokenized_train.map(group_texts, batched = True, batch_size = 1000)\n",
    "lm_test = tokenized_test.map(group_texts, batched = True, batch_size = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] when boiling butter, when it's ready, you can [SEP] pour it into a jar [SEP] [CLS] to permanently attach metal legs to a chair, you can [SEP] weld the metal together to get it to stay firmly in place [SEP] [CLS] how do you indent something? [SEP] leave a space before starting the writing [SEP] [CLS] how do you shake something? [SEP] move it up and down and side to side quickly. [SEP] [CLS] clean tires [SEP] pour water, scrape off caked on dirt. use a steel wool to clean out crevices and narrow spaces. [SEP] [CLS] how do you taste something? [SEP] place it in your mouth to taste\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lm_train[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = lm_train.shuffle(seed=42).select(range(100))\n",
    "small_eval_dataset = lm_test.shuffle(seed=42).select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cjj90\\anaconda3\\envs\\pset2\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 100\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 39\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17f7c9b799bf44b6a383f2ac83eecaee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b5e7d1765d44a39b4ddb2ded9496be3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 8.992371559143066, 'eval_runtime': 21.4162, 'eval_samples_per_second': 4.669, 'eval_steps_per_second': 0.607, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70420d84007e40f8b5698f946922dbe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 8.646451950073242, 'eval_runtime': 18.8526, 'eval_samples_per_second': 5.304, 'eval_steps_per_second': 0.69, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbe84fd1334e42248cc748ad614e02ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 8.52519702911377, 'eval_runtime': 18.2565, 'eval_samples_per_second': 5.478, 'eval_steps_per_second': 0.712, 'epoch': 3.0}\n",
      "{'train_runtime': 247.6237, 'train_samples_per_second': 1.212, 'train_steps_per_second': 0.157, 'train_loss': 8.976375482021234, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=39, training_loss=8.976375482021234, metrics={'train_runtime': 247.6237, 'train_samples_per_second': 1.212, 'train_steps_per_second': 0.157, 'train_loss': 8.976375482021234, 'epoch': 3.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    f\"{model_name}-piqa\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate = 2e-5,\n",
    "    weight_decay = 0.01,\n",
    "    push_to_hub = False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = small_train_dataset,\n",
    "    eval_dataset = small_eval_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 100\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ebc7248586f40bcbf7c1a2aa9c2da3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 5040.18\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in bert-base-uncased-piqa\\tokenizer_config.json\n",
      "Special tokens file saved in bert-base-uncased-piqa\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('bert-base-uncased-piqa\\\\tokenizer_config.json',\n",
       " 'bert-base-uncased-piqa\\\\special_tokens_map.json',\n",
       " 'bert-base-uncased-piqa\\\\vocab.txt',\n",
       " 'bert-base-uncased-piqa\\\\added_tokens.json',\n",
       " 'bert-base-uncased-piqa\\\\tokenizer.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(f\"{model_name}-piqa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a speedrun is a playthrough of a video game, or section of a video game, with the goal of completing it as fast as possible. speedruns often follow planned routes, which may incorporate sequence breaking, and might exploit glitches that allow sections to be skipped or completed more quickly than intended. what is the [SEP] how [SEP] [SEP] [SEP] [SEP] [SEP] [SEP] josiah [SEP] [SEP] [CLS] [SEP] [CLS] [SEP] [SEP] [SEP] [CLS] [SEP] can [SEP] [SEP] to [SEP] [SEP]⁰ [SEP] [CLS] [SEP] [SEP] almond [SEP]\n"
     ]
    }
   ],
   "source": [
    "start_text = (\"A speedrun is a playthrough of a video game, \\\n",
    "or section of a video game, with the goal of \\\n",
    "completing it as fast as possible. Speedruns \\\n",
    "often follow planned routes, which may incorporate sequence \\\n",
    "breaking, and might exploit glitches that allow sections to \\\n",
    "be skipped or completed more quickly than intended. \")\n",
    "\n",
    "prompt = \"What is the\"\n",
    "inputs = tokenizer(\n",
    "     start_text + prompt,\n",
    "     add_special_tokens=False,\n",
    "     return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "prompt_length = len(tokenizer.decode(inputs[0]))\n",
    "outputs = model.generate(\n",
    "     inputs,\n",
    "     max_length=100,\n",
    "     do_sample=True,\n",
    "     top_k=50,\n",
    "     top_p=0.95,\n",
    "     temperature=0.9,\n",
    "     num_return_sequences=3)\n",
    "\n",
    "generated = prompt + tokenizer.decode(outputs[0])[prompt_length + 1:]\n",
    "\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pset2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a8a6b01770c3d88b8610aad9d6cc8ea8e842c992ec484188e2014655869ca619"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
